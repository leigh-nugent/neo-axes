# Data and methods

## The dataset

This dataset consists of drawings and record sheets relating to 1919 Neolithic stone axes found in Britain, compiled by Mike Pitts from 1977-1978. The record sheets contain 56 measurements, for example blade width, poll width, maximum length and weight, among others, in addition to provenance information, the museum collection which the axe is from, and general comments. The record sheets were filled in by hand and later scanned to jpg files. The drawings were also drawn by hand and later scanned to jpg files. This dataset formed the data used for Mike Pitts’ paper in 1996, which set out to group the axes based upon their dimensions using multivariate statistics such as principal components analysis and cluster analysis.  
The raw data in this dataset has its own biases and difficulties which are worth mentioning. Firstly, it was created and curated by one person. Mike Pitts chose and traveled to several museums, at which he recorded all of the complete axes, and some others of varying condition based on his choosing (Pitts 1996, p328). In addition, some record sheets for complete axes have blank fields for a number of measurements. The sheet template also differs in format and order of fields. Due to the nature of human handwriting, some words or numbers may be ambiguous, making data collection more difficult. Another point is that the appearance of the axes is captured through drawings, as opposed to photographs. Cameras were good enough at the time to facilitate this, however it seems as though Pitts thought drawing the axes would preserve more detail (Pitts 1996 p328). This does, however, introduce further bias, as obviously the images are one person’s interpretation of a physical object. What is more, Pitts also says that the drawings were created away from the museums and not whilst viewing the axes, but rather are based on the measurements in the record sheets taken when handling the artefacts (p328). The drawings are also on the other side of the same sheet of paper that the record sheets are printed on, which may cause problems with image processing as lines are faintly visible from the other side. They also seem to have been drawn in pencil and some of the lines are very faint. 

## Makefile

A Makefile has been used to automate the building of this project and simplify it for easier reproducibility. The beauty of this is that it can run many files and control the order of their execution, simplifying the build. This eliminates the need to provide extensive instructions on the order to execute the files in. In addition, once ran, Makefiles only create files which are not already made, saving time on each execution (Marwick et al. 2018 p82). This entire project can be built by running the command `make` using the terminal inside the top level of the dissertation folder. GNU Make is already part of the software on Unix systems, however will need to be purposely installed if using Windows. The Makefile has ‘targets’, which are files it needs to create, ‘dependencies’, which are prerequisite files that are needed to make the targets, and ‘recipes’, which tell the Makefile how to make the targets.

## Data extraction

### Extracting record sheet data

The method initially considered for digitizing the data from the record sheets was to use a handwriting recognition model. I found a handwriting recognition model written in Python and implemented with TensorFlow, which took single words/numbers (from GitHub: githubharald/SimpleHTR). The model was chosen as it looked straightforward, had a good ReadME, and had been starred by other GitHub users 963 times. In order to get the record sheet data into this format, however, they needed to be split into as many images as there were measurement fields. For the more qualitative fields such as the comments, either something else would need to be considered, or perhaps they would be left out. This was attempted using homography and brute force matching with the python-opencv library in Jupyter Notebooks to find the coordinates of the record sheet grids, which would then be used to crop the sheets, so that each image was split into 56 squares (please see Appendices). This method successfully found the grid lines, and was able to crop one of the grid squares. In addition, to check that the model would work on this dataset, some single numbers were manually cropped and fed into the model. These were all predicted incorrectly, mainly with decimal points being omitted, or ‘5’ being mistaken as an ‘S’. This would most likely have been improved considerably by training the model on the handwriting in this dataset, however this would have required manually transcribing a large number of the record sheets to use as labels for the training data. Eventually this method was decided against as it would take too long to ensure this method worked successfully, especially as everything returned by the model would need to be checked, which would be a mammoth task.

Another HTR method is Transkribus, launched in 2015 to help digitize historical archives as part of the Transcriptorium project. This is, however, mainly used for historical documents which are usually solely lines of handwriting (Muehlberger 2019, p 955). As of this year, larger datasets require a fee for processing. This also had the problem of needing a training dataset, which would mean transcribing a large number of the record sheets to provide labels for the sheets used in training. In addition, the returned data would also need to be checked for accuracy. Again, this method was decided against due to time constraints.

After considering these deep learning methods, it was decided that transcriptions of the record sheets would be crowdsourced instead. This would enable more time to be spent on processing the axe drawings, as this would take much longer to do manually using PhotoShop. The crowdsourcing platform chosen was MicroPasts as it is specifically for archaeological projects. MicroPasts runs on Pybossa, which uploads projects and tasks to the platform, and has an API which can be used via the command line. The tasks, in this case the record sheet images, needed to be hosted on another website and linked to using URLs imported to MicroPasts with Pybossa. To do this, the sheets first needed to be separated from the drawings, and any sheets which did not have a drawing associated, or had duplicated axe ids in their filenames, were omitted. This left 1825 pairs of record sheets and drawings, each representing one axe. These were separated into a separate folder and their filenames were saved to csv by `image-organiser.R`. They were then uploaded to a GitHub repository and the filenames imported into an R dataframe in `task-generator.R`, which then joined the filenames with the GitHub URL to create their individual URLs and exported to csv for Pybossa to import. 

To create the project, the template HTML from the Bronze Age Index Card transcription project on MicroPasts’ GitHub was modified to show the 63 fields in the record sheets and a tutorial. This template was chosen as it had a similar input form which could be modified for this project easily. Once this was modified to the needs of this project, most fields were single-line, however the comments and museum labels were multiple-line fields. It was then important to provide detailed instructions on how the transcribers should fill out the forms provided due to some ambiguous aspects of the record sheets. Firstly, the field names were abbreviated variables which needed to be defined so that the volunteers knew what they were entering, and the units of measurement for these were also provided to help discern how large or small a number was. In addition, volunteers were asked to enter ellipses in square brackets next to any transcribed text that they were unsure about to ensure it was found during data cleanup. The instructions also asked volunteers to make sure all characters were transcribed, even if it did not make sense, for example brackets around numbers. This was because this project is aiming to preserve as much of the original information as possible, which be used for training data at a later date. Below is a screenshot of the form designed, alongside one of the images for transcription, or ‘tasks’ (figure).

To retrieve the data from MicroPasts, a separate Makefile was used to download 100 tasks at a time into jsons in order to bypass Pybossa’s pagination limit.

Due to time constraints and the volume of axes in the dataset, the redundancy was set to 1 for all tasks. This means that each sheet was only transcribed once, by one person, so there was only one chance to correctly enter the data on the crowdsourcing platform. Once all sheets were transcribed, a period of time was spent making corrections to data entry errors and missed data. The first part of this cleanup process involved creating a csv of all axes with missing national grid references. This was done by attempting to convert them into latitudes and longitudes using the `osgparse` function in the `rnrfa` R package, and `anti_join` was used to see which axes did not appear in the coordinates dataframe made by `rnrfa`, but did appear in the data imported from micropasts-neoaxes1.csv. These were then exported to csv and checked against their original record sheets, with 50 of these being actually incorrect, and the rest having been missing from the original sheets initially. The second strategy for identifying data entry errors was to examine each column in the micropasts-neoaxes1 csv and order them by ascending and descending to spot atypically low or high values. A correction sheet csv file was made with the axe number, feature to change, and the new value, which was then imported and `left_join`ed back to the imported Micropasts data to correct it. This way the corrections are logged and reproducible. 

### Processing the axe drawings

Since the majority of analyses involving calculations performed on the geometry of shapes in archaeology extracted their data manually, this methodology sets out to not only automate these processes, but also make them reproducible. `Momocs` requires raster silhouettes - black shapes on white backgrounds - for input, and takes one shape per image. In addition, these shapes need to be similar enough to compare (Bonhomme et al 2014, pxxx). Each axe drawing usually shows 3 views: plan, profile and top, so to compare effectively, each type would need to be analysed with others of the same view. 

Workflow for image processing from pencil outline to filled binary image is as follows: 
view-finder.py: find threshold, dilate threshold to join lines (differing pencil pressure created gaps); use the edges in this to draw contours; use contours to find bounding boxes of each angle; create list of bounding box coordinates for each angle then export to CSV with filename, new filename, index no. (figure: drawing with contours and bounding boxes drawn)

The axe cropping is done in the Makefile using a formula which is replaced with the coordinates for the bounding boxes (formula from Makefile).

`axe-filler.py`: import each individual image file of axe views and threshold; dilate threshold by 2 iterations (this will distort the shape of the axe by connecting the outline to the noise around it); find contours and fill them in; remove all contours which have a smaller area than the largest contour, which will be the axe shape; erode the axe shape so that the rest of the noise connected to the largest contour becomes disconnected and is no longer shown, as it is not part of the largest contour, and eroding reveals the original axe outline. Finally, inverts colours so that the axe shape is a black silhouette on white mask (figure: filled plan, final image).

```{python eval=FALSE, python.reticulate=FALSE}
#draw the contours onto the axe angles, trying to find curves
contour_list = []
for contour in contours:
    approx = cv2.approxPolyDP(contour,0.01*cv2.arcLength(contour,True),True)
    area = cv2.contourArea(contour)
    if ((len(approx) > 4) & (area > 60) ):
        contour_list.append(contour)

#get list of boxes
boxes = []
contour_areas = []
for c in contours:
    rect = cv2.boundingRect(c)
    if rect[2] < 100 or rect[3] < 100: continue
    x,y,w,h = rect
    list(rect)
    boxes.append(rect)
    area = cv2.contourArea(c)
    contour_areas.append(area)
```


## Analysis

### Size morphometrics with axe measurement data

The statistical treatment of the axe measurements involved methods which were trialed on 100 randomly selected axes from this dataset for a previous assignment towards the module ARCL0087 earlier this year. The definitions of these variables can be found in the glossary, and a diagram of the features on each axe is below (mikepittsfigure). First, a dotplot was used to visualise which the frequency of measurements for each feature, and the amount of axes with each condition type. After this, features which were not recorded for most of the axes were removed. Density and violin plots were then used to preliminarily explore the distributions of the variables and observe the influence of outliers. Boxplots were initially considered, however as they can be misleading when used alone on multimodal or very leptokurtic distributions (Baxter 2015, p30), violin plots were chosen to ensure clarity for other types of distribution. This is especially useful as it is predicted or even hoped that some distributions will be multimodal since this investigation is looking for groups. After this, PCA was performed to reduce dimensions and reveal patterns in this rather large multivariate dataset. This was chosen as PCA shows relationships between different variables e.g. short and squat vs tall and narrow, which is helpful for shape analysis and grouping. PCA is especially useful for multivariate datasets where many of the variables correlate, as groups of correlated variables can be separated, which in turn can help to distinguish groups and discern which variables define groups, if any (Shennan 1997, p297). The `prcomp` function in the R stats package was used for direct comparison with Momocs. Because PCA does not work with missing values, only the most commonly measured variables will be chosen, and any axes with measurements missing for these will have them imputed using the missMDA package. This R package was chosen as its impute_pca method is best for highly correlated variables, which is the case with this dataset (Dray and Josse 2015, p665). The method for calculating the variable loadings from a `prcomp` object has been taken from Rhys 2019 (p332) and uses the `map_dfc` function, returning a tibble. Scree plots were also examined to determine the number of components which explain the most variance. 
Once the measurements responsible for the most variation have been identified in PCA, k-means clustering will be used to visualise any groups. K-means was chosen instead of other clustering methods as it is nonhierarchical and does not depend on ordinal or presence/absence data, none of which this apply to this dataset. To help decide the optimal amount of clusters to set in the k-means analysis, the total within cluster sum of squares (WSS) and average silhouette were plotted. Often it is best to compare multiple k-means solutions as it can be difficult to discern the optimal amount of clusters to use (Baxter 2015, p147), so all options suggested by the WSS and silhouette plots will be explored. Combining these analyses can give a good insight into potential groups which may exist in the data. This workflow was also chosen for comparison with Pitts’ analyses on the same data. 

### Clustering shapes in the axe drawings

The Keras library in Python, which allows Python to interface with the deep-learning library Tensorflow, was initially considered for grouping the axe drawings with unsupervised clustering. Deep learning in recent years has been trialed in archaeological remote sensing to help automate the identification of features in large image datasets (Küçükdemirci & Sarris 2020, p108). This made Keras seem particularly attractive for revealing patterns in the axe drawing dataset which is too large to inspect manually. However, deep learning methods are best suited for highly dimensional datasets, which remote sensing data is a good example of, however the axe drawings are relatively simple outlines. The ability of deep learning models to deal with such complexity can make them susceptible to overfitting (Patel 2019 iii 8), which would most likely happen with a dataset of simpler images such as this one. Because the drawings in this dataset are so simple, it would make sense to treat them geometrically as opposed to an entire image.

The Momocs package for R (Bonhomme 2013) was instead chosen to compare the axe drawings based on analysing their outlines morphometrically, since this was more comparable with the morphometric analysis of the measurements. This entails quantifying the shapes for comparison. Momocs was much better suited to this dataset as it deals very well with simple outlines. As morphometric analysis is the statistical treatment of size and shape (Claude 2008, p1), and the size of axes has already been compared, this analysis deals with shape, providing a more complete morphometric analysis of the axes.  This version of Momocs was chosen despite a newer version being released this year as there is more extensive documentation for it currently. 

Momocs uses Fourier transforms to quantify shapes with closed outlines for statistical analysis. Closed outlines, as opposed to open outlines which have a definitive start and end point, are continuous, and because of this can be described by periodic functions (Bonhomme 2013 p5). Fourier transform methods break down these periodic functions into a sum of simpler, weighted functions (Claude 2008 p213). It is similar to reducing dimensions in PCA. Of the three different types of Fourier transform offered by Momocs, elliptical Fourier transform, which fits curves to closed outlines, will be used. It was chosen as it does not require equally spaced points or a regular outline, can be fitted to almost any shape (Bonhomme et al. 2014 p7, Claude 2008, p221), coefficients can be made regardless of outline position, and the coefficients are normalized meaning that prior alignment steps are usually not necessary (Rohlf and Archie 1984 p315). Momocs’ `efourier` function uses the algorithm for elliptic Fourier analysis developed by Giardina and Kuhl (1977 p277) and Kuhl and Giardina (1982 p238-9) . `efourier` transforms each outline into a list of four coefficients that describe different aspects of the shape, which are then multiplied by the number of harmonics chosen (Bonhomme et al. 2014 p14). These outlines are then in a quantified form suitable for multivariate analyses.

All axes were aligned the same way before importing into Momocs, with their blades at the bottom and their polls at the top. Before perfoming any analyses on the shapes, outlines were checked for correctness by using the Momocs function `panel` and displaying the names as just the ID numbers from each filename over each shape, extracted from the character vector of filenames using regex. All erroneous shapes were then removed from the list of filenames to import into Momocs.  

`efourier` was used with its default normalize setting, which is not appropriate for shapes with strong bilateral symmetry or that are very circular, however the shapes in this dataset are reasonably irregular and more elliptical, so this option should be safe in this instance (Momocs documentation). The method of deciding how many harmonics to use is up to the user and is a topic of debate in the literature (Bonhomme et al. 2014 p12). The number of harmonics represents the number of iterations around the shape it takes to reconstruct it, the first harmonic always being an ellipsis in elliptical Fourier analysis. To determine the number of harmonics to choose, firstly `harm_pow` was used to plot the harmonic power of the first, middle and last axes in the dataset, and secondly `hcontrib` on the overall dataset. The former function shows a line plot of how much of the shape has been reconstructed by each harmonic, and also shows at which point this stops improving, which is where the line plateaus. `hcontrib` shows the appearance of a mean axe shape when reconstructed from an increasing number of harmonics. 

For analysing the Fourier-transformed outlines, `PCA` and `KMEANS` were used, which are Momocs functions that use the R stats library packages `prcomp` and `kmeans` but are also able to plot mean shapes on a morphospace. In PCA, these mean shapes show a typical axe at intervals on each principal component, and in k-means, these show mean shapes for each cluster. Finally `panel` will be used with the k-means clusters colour-coded onto the shapes to show the membership of the clusters in more detail.