```{r methods-packages, echo=FALSE, include=FALSE}
#library(reticulate)
library(knitr)
library(tidyverse)
axe_dpi <- 350L
#use_virtualenv(here::here(".venv"), required = TRUE)
#knit_engines$set(python = reticulate::eng_python)
```


# Data and methods

## The dataset

This dataset consists of drawings and record sheets relating to 1919 Neolithic stone axes found in Britain, compiled by Mike Pitts in his spare time from 1977-1978. The record sheets contain 56 measurements, for example blade width, poll width, maximum length and weight, among others, in addition to provenance information, the museum collection which the axe is from, and general comments. It is a feat that one person was able to catalogue this much information, even if some is missing, as will become apparent in later sections. In addition, the associated drawings are very useful and rarely provided. Abbreviated variable names in the sheets were coined by Pitts and their definitions can be found in the glossary of this dissertation. Most record sheets have an associated drawing for the same axe. Both the drawings and the record sheets were hand-drawn or written, and seemingly in pencil. These were then scanned to JPG files in 2016 by the Implement Petrology Group. Mike Pitts used this dataset in his own set of 'numerical typology' analyses, similar to the workflow in this project, and he reported the outcome as inconclusive. This was because the analyses did not separate axes into known archaeological groups [@pitts1996 339].
This raw data has its own biases and difficulties which are worth mentioning. Firstly, it was created and curated by one person. Pitts chose and traveled to several museums, at which he recorded measurements for all of the complete axes, and some others of varying condition based on his choosing [@pitts1996 328]. In addition, some record sheets for complete axes have blank fields for a number of measurements. The sheet template also differs in format and order of fields. Due to the nature of human handwriting, some words or numbers may be ambiguous, making data collection more difficult. Another point is that the axes are drawn rather than photographed. Cameras were good enough at the time to facilitate this, however it seems as though Pitts’ aim in drawing the axes was to preserve more detail [@pitts1996 328]. This does, however, introduce further bias, as obviously the images are one person’s interpretation of a physical object. What is more, Pitts also says that the axes were sketched based on measurements in the record sheets taken when handling the artefacts, and not whilst in the museum observing them [@pitts1996 328]. The drawings are also on the other side of the same sheet of paper that the record sheets are written on, which may cause problems with image processing as lines are faintly visible from the other side. A final point is that because these are drawn in pencil, there are gaps in the lines due to differing placement of pressure.

## Makefile and Dockerfile

A `Makefile` [@make] has been used to orchestrate the building of this project in a simple manner for easier reproducibility. This also proved to be invaluable in managing the complexity of the project which involved thousands of source files, multiple programming and command languages (R, Python, AWK, Bash, jq) alongside command-line programs (curl and xargs among others). The Makefile has ‘targets’, which are files it needs to create, ‘dependencies’, which are prerequisite files that are needed to make the targets, and ‘recipes’, which tell the Makefile how to make the targets. In this way, the syntax of the Makefile defines what needs to be done, but the order of particular commands is controlled dynamically by the GNU Make program. This declarative style complemented the procedural style used in the project's scripts well. It eliminated the need to provide extensive, step-by-step, instructions on the order to execute the scripts and on which files, indeed the syntax of the `Makefile` in fact self-documents the dependencies in the project. For example, once ran, Make only create files which do not already exist, or updates those files whose dependencies have been modified, saving time on each execution [@marwick2018 82]. It also allowed the most computationally intensive part of the project, the image processing steps with OpenCV, to be ran in parallel (using the `-j` argument) to take advantage of the multiple cores on a modern CPU. The entire project can be built by running the command `make` inside the top level of the dissertation folder (but only if the required system dependencies are available).

The need for a reproducible Linux environment containing the extensive system dependencies led to the use of Docker. A `Dockerfile` has been provided to build a Docker image, it is based on the `rocker/geospatial:4.0.3` image [@rocker_geospatial], with additional system packages, TeX Live packages [@texlive], the Python `venv` and the `renv` R package. The `renv` itself is created when the Docker image is run so that the packages are downloaded once and cached within a mounted volume, saving time when the image is rebuilt. Recipes to build and run the Docker image are included within the `Makefile` targets: `make build` and `make run`.  The recipe of the latter ensures the mounted volumes have the same user and group permissions as the Docker host, allowing for seamless access to and from the host's filesystem. The recommended way to reproduce this project is to first run `make clean` to remove the ready-made files provided, then `make build` and `make run`.

## Data extraction

### Extracting record sheet data

The method initially considered for digitizing the data from the record sheets was to use a handwriting recognition model. A handwriting recognition model was found on GitHub, which was written in Python and implemented with TensorFlow, and took single segmented words or numbers [@scheidl2020]. The model was chosen as it looked relatively straightforward, had a good README, and had been starred by other GitHub users 963 times. In order to get the record sheet data into this format, however, they needed to be split into as many images as there were measurement fields. For the more qualitative fields such as the comments, either something else would need to be considered, or perhaps they would be left out.  Segmentation was attempted by using `HoughLines` from the `opencv-python` [@opencv] library in `Python` [@python] to draw lines onto the grids, the coordinates of which would then be used to crop the sheets, so that each image was split into 56 squares (please see `appendices/hough-transform.ipynb` and `appendices/hough-transform-copy1.ipynb`). This method successfully found the grid lines, however after this point was reached, HTR was decided against due to the expected time and effort it would take to ensure it worked satisfactorily. This would have required transcribing a large number of the record sheets to use as labels for the training data. In addition, everything returned by the model would need to be checked, which would be a mammoth task.

After considering this deep learning method, it was decided that transcriptions of the record sheets would be crowdsourced instead. This would enable more time to be spent on processing the axe drawings, as this would take far longer to do manually using a raster graphics editor. The crowdsourcing platform chosen was MicroPasts as it is specifically for archaeological projects. MicroPasts uses Pybossa for crowdcrafting, which uploads projects and tasks to the platform, and has an API which can be accessed via the command line. The tasks, in this case the record sheet images, needed to be hosted on another website and linked to using URLs imported to MicroPasts with Pybossa. To do this, the sheets first needed to be separated from the drawings, and any sheets which did not have a drawing associated, or had duplicated axe IDs in their filenames, were omitted. This left 1825 pairs of record sheets and drawings, each pair representing one axe. These were placed into separate folders and their filenames and paths were saved to csv. The images were then uploaded to a GitHub repository and the filenames from the csv were imported into R and joined with the GitHub URL to make their individual URLs, which were then exported to csv for Pybossa to import.

To create the project, the template HTML from the Bronze Age Index Card transcription project on MicroPasts [@pett2018] was modified to show the 63 fields in the record sheets and a tutorial. This template was chosen as it had a similar input form which could be easily modified for this project. Below is a screenshot of the transcription form, alongside one of the record sheet images, or ‘tasks’ (Figure \@ref(fig:mp-form-screenshot)). 

```{r mp-form-screenshot, echo=FALSE, fig.align="center", fig.cap="Screenshot of an example transcription form with sheet."}
include_graphics("figure/mp-form-screenshot.png", dpi= 200L)
```

It was then important to provide detailed instructions in the tutorial on how the transcribers should enter the data as some aspects of the sheets were ambiguous. Firstly, the field name abbreviations were defined to prevent confusion, and the units of measurement were provided to aid in ascertaining correct data entry. In addition, volunteers were asked to enter an ellipsis in square brackets next to any dubious transcribed text so that it could later be found during data cleanup. The instructions also asked volunteers to make sure all characters were transcribed, even if it did not make sense, for example brackets around numbers. This was because this project is aiming to preserve as much of the original information as possible for potential future use as training data. 

To retrieve the data from MicroPasts, a separate Makefile (`micropasts.mk`), which the main Makefile also runs, was used to download 100 tasks at a time into jsons in order to bypass Pybossa’s pagination limit. This communicates with the API and gives it the instructions to limit downloads to 100 and 'offset' them, so that each 100 is really 0-99 and so on. This Makefile also runs the R script (`scripts/sheets.R`) responsible for creating the final `data/micropasts-neoaxes1.csv` data for import into the analysis. This script removes all non-numerical characters from the numerical fields and imports the data entry error corrections from `data/correction-sheet.csv`, which is explained below.

Due to time constraints and the volume of axes in the dataset, the redundancy was set to 1 for all tasks. This means that each sheet was only transcribed once, by one person, so there was only one chance to correctly enter the data on the crowdsourcing platform. Once all sheets were transcribed, a period of time was spent making corrections to data entry errors and missed data. The first part of this cleanup process involved creating a csv of all axes with missing national grid references.  This was done by attempting to convert them into latitudes and longitudes using the `osgparse` function in the `rnrfa` [@rnrfa] R package and creating a dataframe with their lat-longs and axe Ids. Any which did not successfully convert were found using `anti_join` from `tidyverse` [@tidyverse] to see which axe IDs appeared in the MicroPasts data but not in the coordinates dataframe. These were then exported to csv and checked against their original record sheets, with 50 of these being actually incorrect, and the rest having been missing from the original sheets initially. The second strategy for identifying data entry errors was to examine each column in `data/micropasts-neoaxes1.csv` and order them by ascending and descending to spot atypically low or high values. The `data/correction-sheet.csv` file was made with the axe number, feature to change, and the new value, which was then imported and joined back onto the imported Micropasts data to correct it. This way the corrections are logged and reproducible. 

\pagebreak

### Processing the axe drawings

Since the majority of analyses involving calculations performed on the geometry of shapes in archaeology extracted their data manually, this methodology sets out to not only automate these processes, but also make them reproducible. `Momocs` requires raster silhouettes - black shapes on white backgrounds - for input, and takes one shape per image. In addition, these shapes need to be similar enough to compare [@bonhomme2014 5, 11]. Each axe drawing usually shows 3 views: plan, profile and top (Figure \@ref(fig:example-drawing)), so to compare effectively, each type would need to be analysed with others of the same view. The workflow for image processing from pencil outline to filled binary image is as follows.

```{r example-drawing, echo=FALSE, fig.align="center", fig.cap="Example drawing of axe 10031."}
include_graphics("drawings/10031d.jpg", dpi = axe_dpi)
```

\pagebreak

The first step was to crop the drawings into single views, which is carried out by `scripts/view-finder.py`. This was done by first finding the contours of the drawing outlines. Contours are found by using edge detection on an image, then finding the contours of the edge-detected image. Because the pencil drawings are faint in some areas, the lines have gaps, which creates problems finding the entire shape. To mitigate against this, the edge-detected image was dilated so that the edges were thicker, forcing them to join, and making the contours larger (code snippet; Figure \@ref(fig:draw-contours)).

```{python find-contours, eval=FALSE}
#find the edges of the drawing
edge_detected_image = cv2.Canny(imgray, 1, 100)
#increase width of edges forcing them to join
kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(9,9))
dilated = cv2.dilate(edge_detected_image, kernel)
#find contours of dilated edge-detected image
contours, hierarchy = cv2.findContours(dilated.copy(), 
                    cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
outlines = segments.copy()
#append curved contours to list
contour_list = []
for contour in contours:
    approx = cv2.approxPolyDP(contour,0.01*cv2.arcLength
            (contour,True),True)
    area = cv2.contourArea(contour)
    if ((len(approx) > 4) & (area > 60) ):
        contour_list.append(contour)
#not included in final script, used in development to see contours
cv2.drawContours(outlines, contour_list, -1, (255,0,0))
```


```{r draw-contours, echo=FALSE, fig.align="center", fig.cap="Dilated contours found from edge-detecting the drawing, then superimposed onto the drawing"}
include_graphics("figure/10031-contours.jpg", dpi = axe_dpi)
```

Once the contours were found, it was possible to find the bounding boxes of each view (code snippet; Figure \@ref(fig:bounding-boxes)). The coordinates for each box were appended to a list of lists, which was then converted to a `pandas` dataframe for export to csv. 

```{python find-boxes, eval=FALSE}
for c in contours:
    rect = cv2.boundingRect(c)
    if rect[2] < 100 or rect[3] < 100: continue
    print(cv2.contourArea(c))
    x,y,w,h = rect
    cv2.rectangle(outlines,(x,y),(x+w,y+h),(0,255,0),2)
```


```{r bounding-boxes, echo=FALSE, fig.align="center", fig.cap="Bounding boxes drawn around each view."}
include_graphics("figure/10031-boxes.jpg", dpi = axe_dpi)
```

This dataframe also included the area and aspect ratio of each view, which was used to determine which view had been cropped, and this was used in the filename (code snippet). The dataframe is exported to csv for each axe, which the Makefile looks in for recipe instructions. 

\pagebreak

```{python construct-df, eval=FALSE}
#function to name views based on logic
def nameview(ca, max_ca, min_ca, ratio, max_r, min_r):
    if ca == max_ca:
        return "plan"
    elif ratio == max_r:
        return "profile"
    elif ca == min_ca and (min_r > ratio < max_r) :
        return "top"
    else:
        return "tbc"
    
#need to vectorise above function as pandas deals in vectors
vectfunc = np.vectorize(nameview, cache=False)

df = pd.DataFrame(boxes, columns=['x', 'y', 'w', 'h'])
df['axe'] = df.index
df['drawing'] = image
df['area'] = df['h'] * df['w']
df['aspect_ratio'] = df['h'] / df['w']

#using vectorised function above to name the views
df['view'] = vectfunc(df['area'], max(df['area']), min(df['area']), \
            df['aspect_ratio'], max(df['aspect_ratio']), \
            min(df['aspect_ratio']))

df['filename'] = df.drawing.str.replace('d\.jpg','').str.cat \
            ('-'+df.view.astype(str)+'-'+df.axe.astype(str) \
            +'.jpg')
```

\pagebreak

The `Makefile` crops the views using the `convert` program from `imagemagick` and arguments are given to it which are replaced with the coordinates for the bounding boxes (please see `Makefile`). Finally, the cropped axe views are filled in by `scripts/fill_view.py` in three steps. The outline is first filled by finding the contours again, but this time filling them when drawing over the image (code snippet; Figure \@ref(fig:filled-conts)).

```{python fill-cont, eval=FALSE}
cv2.drawContours(outlines_copy, contour_list, -1, (0,0,0), \
                thickness=cv2.FILLED)
```

```{r filled-conts, echo=FALSE, fig.align="center", fig.cap="Filled contours."}
include_graphics("figure/10031-filled-contours.jpg", dpi = axe_dpi)
```

The largest contour was then found, which is the axe shape, and only that was drawn onto the image, removing other contours which were not connected to it (code snippet; Figure \@ref(fig:largest-cont)).

```{python draw-largest-cont, eval=FALSE}
#using contour area to find largest contour
maxcont = max(contours, key = cv2.contourArea)
large_cont = []

#if contour area is larger than or equal to max, append
for cont in contours:
    if cv2.contourArea(cont) >= cv2.contourArea(maxcont):
        large_cont.append(cont)
        
outlines_copy() = outlines.copy()

#creates image with largest contour only
cv2.drawContours(outlines_copy, large_cont, -1, (0,0,0), \
thickness=cv2.FILLED)
```

```{r largest-cont, echo=FALSE, fig.align="center", fig.cap="Only the largest contour is now drawn."}
include_graphics("figure/10031-largest-contour.jpg", dpi = axe_dpi)
```

Finally, the now-filled largest contour was thresholded so that `cv2.MORPH_OPEN` could be used on it to dilate the contour and remove the connected nearby shapes from its outline (code snippet; Figure \@ref(fig:final-fill)).

```{python final-filled, eval=FALSE}
#thresholding the filled largest contour
thresh, outlines_thresh = cv2.threshold(outlines4, 250, 255, \
                            cv2.THRESH_BINARY_INV)
 
#dilating the thresholded contour
closing = cv2.morphologyEx(outlines_thresh, cv2.MORPH_OPEN, \
            kernel, iterations=10)
            
#final silhouette colours inverted            
silhouette = cv2.bitwise_not(closing)
```

```{r final-fill, echo=FALSE, fig.align="center", fig.cap="Final filled axe shape."}
include_graphics("figure/10031-final-fill.jpg", dpi = axe_dpi)
```

\clearpage

## Analysis

### Size morphometrics with axe measurement data

The statistical treatment of the axe measurements involved methods which were trialed on 100 randomly selected axes from this dataset for a previous assignment toward the module ARCL0087 earlier this year. A diagram with these variables labeled on an axe can be found in Pitts’ paper [@pitts1996 331]. First, a dotplot was used to visualise the frequency of measurements for each feature and the amount of axes with each condition type. After this, features which were not recorded for most of the axes were removed. Density and violin plots were then used to preliminarily explore the distributions of the variables and observe the influence of outliers. Boxplots were initially considered, however as they can be misleading when used alone on multimodal or very leptokurtic distributions [@baxter2015 30], violin plots were chosen to ensure clarity for other types of distribution. This is especially useful as it is hoped that some distributions will be multimodal since this investigation is looking for groups. 

After this, PCA was performed to reduce dimensions and reveal patterns in the axe measurement data. PCA was chosen as it shows relationships between different variables (for example short and squat versus tall and narrow), which is helpful for shape analysis and grouping. PCA is especially useful for multivariate datasets where many of the variables correlate, as groups of correlated variables can be separated, which in turn can help to distinguish groups and discern which variables define groups, if any exist [@shennan1988 297]. The `prcomp` function in the R stats package was used for direct comparison with Momocs.  Because PCA does not work with missing values, any axes with measurements still missing after only keeping the most measured variables will have them imputed using the `missMDA` package [@missmda]. This R package was chosen as its `impute_pca` method is best for highly correlated variables, which is the case with this dataset [@dray2015 665]. The method for calculating the variable loadings from a `prcomp` object has been taken from Rhys (2019) and uses the `map_dfc` function from the `tidyverse` package to return a tibble [@rhys2019 332]. Scree plots were also examined to determine the number of components which explain the most variance. 

Once the measurements responsible for the most variation have been identified in PCA, k-means clustering was used to help reveal any existing groups. K-means was chosen instead of other clustering methods as it is nonhierarchical and does not depend on ordinal or presence/absence data, neither of which this apply to this dataset. To help decide the optimal amount of clusters to set in the k-means analysis, the total within cluster sum of squares (WSS) and average silhouette were plotted. Often it is best to compare multiple k-means solutions as it can be difficult to discern the optimal amount of clusters to use [@baxter2015 147], so all options suggested by the WSS and silhouette plots will be explored. Combining these analyses can give a good insight into potential groups which may exist in the data. This workflow was also chosen for comparison with Pitts’ analyses on the same data. 

### Clustering shapes in the axe drawings

The Keras library, which allows Python to interface with the deep-learning library Tensorflow, was initially considered for grouping the axe drawings with unsupervised clustering. Deep learning in recent years has been trialed in archaeological remote sensing to help automate the identification of features in large image datasets [@kucukdemirci2020 108]. This made Keras seem particularly attractive for revealing patterns in the axe drawing dataset which is too large to inspect manually. However, deep learning methods are best suited for highly dimensional datasets, which remote sensing data is a good example of, whereas the axe drawings are relatively simple outlines. The ability of deep learning models to deal with such complexity can make them susceptible to overfitting [@patel2019 iii 8], which would most likely happen with a dataset of simpler images such as this one. 

The `Momocs` package [@bonhomme2014] for `R` was instead chosen to compare the axe drawings by analysing their outlines morphometrically, since this was more comparable with the morphometric analysis using the measurements.  As morphometric analysis is the statistical treatment of size and shape [@claude2008 1], and the size of axes has already been compared, this analysis deals with shape, providing a more complete morphometric analysis of the axes. 

Momocs uses Fourier transforms to quantify shapes with closed outlines for statistical analysis. Closed outlines, as opposed to open outlines which have a definitive start and end point, are continuous, and because of this can be described by periodic functions (Bonhomme 2013 p5). Fourier transform methods break down these periodic functions into a sum of simpler, weighted functions [@claude2008 213]. It is similar to reducing dimensions in PCA. Of the three different types of Fourier transform offered by `Momocs`, elliptical Fourier transform, which fits curves to closed outlines, will be used. It was chosen as it does not require equally spaced points or a regular outline, can be fitted to almost any shape [@bonhomme2014 7], coefficients can be made regardless of outline position [@claude2008 221], and the coefficients are normalized meaning that prior alignment steps are usually not necessary [@rohlf1984 315]. `Momocs`’ `efourier` function uses the algorithm for elliptic Fourier analysis developed by @giardina1977 and @kuhl1982 (p.277 and 238-9, respectively). `efourier` transforms each outline into a list of four coefficients that describe different aspects of the shape, which are then multiplied by the number of harmonics chosen [@bonhomme2014 14]. These outlines are then in a quantified form suitable for multivariate analyses.

All axes were aligned the same way before importing into `Momocs`, with their blades at the bottom and their polls at the top. Before perfoming any analyses on the shapes, outlines were checked for correctness by using the `Momocs` function `panel` and displaying the names as just the ID numbers from each filename over each shape, extracted from the character vector of filenames using regex. All erroneous shapes were then removed from the list of filenames to import into `Momocs`.  

`efourier` was used with its default normalize setting, which is not appropriate for shapes with strong bilateral symmetry or that are very circular, however the shapes in this dataset are reasonably irregular and more elliptical, so this option should be safe in this instance (`Momocs` `efourier` documentation). The method of deciding how many harmonics to use is up to the user and is a topic of debate in the literature [@bonhomme2014 12]. The number of harmonics represents the number of iterations around the shape it takes to reconstruct it, the first harmonic in elliptical Fourier analysis always being an ellipse. To determine the number of harmonics to choose, firstly `harm_pow` was used to plot the harmonic power of the first, middle and last axes in the dataset, and secondly `hcontrib` on the overall dataset. The former function shows a line plot of how much of the shape has been reconstructed by each harmonic, and also shows at which point this stops improving, which is where the line plateaus. `hcontrib` shows the appearance of a mean axe shape when reconstructed from an increasing number of harmonics. 

For analysing the Fourier-transformed outlines, `PCA` and `KMEANS` were used, which are `Momocs` functions that use the R stats library functions `prcomp` and `kmeans` but are also able to plot mean shapes onto a morphospace. In PCA, these mean shapes show a typical axe at intervals on each principal component, and in k-means, these show mean shapes for each cluster. Finally `panel` will be used with the k-means clusters colour-coded onto the shapes to show the membership of the clusters in more detail.

### Drawing comparisons

Once the analyses on both types of data have revealed an ideal number of clusters, these two sets of clusters will be compared to see if there is overlap in membership, and also if they persist geospatially. The `MSHAPES` function from `Momocs` will be used to plot the mean outline from each record sheet data cluster, using the outlines from the drawings associated with the axes in each. A comparison table will also be used to see the percentage of record sheet cluster axes which are present in the outline clusters. Finally, `ggmap` [@ggmap] was used to plot the find places on a map of Britain, which was faceted to show one plot for each cluster, with record sheet clusters on top and outline clusters underneath. `geom_density_2d` from `ggplot2` was used to visualise areas which were highly populated.